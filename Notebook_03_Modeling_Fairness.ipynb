{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Modeling and Fairness Analysis\n",
    "\n",
    "This notebook trains final models, performs comprehensive evaluation, conducts fairness audits, and provides deployment recommendations.\n",
    "\n",
    "**Target**: `arrest_made` (binary)\n",
    "\n",
    "**Sections**:\n",
    "1. Reproducibility Setup\n",
    "2. Load Prepared Data\n",
    "3. Model Training (Logistic Regression & Decision Tree)\n",
    "4. Comprehensive Evaluation\n",
    "5. Interpretability Analysis\n",
    "6. Fairness Audit\n",
    "7. Error Analysis\n",
    "8. Mitigation Experiments\n",
    "9. Final Conclusions & Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reproducibility Setup\n",
    "\n",
    "**WHY**: Ensure identical results across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REPRODUCIBILITY HEADER - NOTEBOOK 03\n",
      "============================================================\n",
      "Timestamp: 2026-01-15T17:49:57.568695\n",
      "SEED: 42\n",
      "Python: 3.10.19 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 16:41:31) [MSC v.1929 64 bit (AMD64)]\n",
      "NumPy: 2.2.6\n",
      "Pandas: 2.3.3\n",
      "Scikit-learn: 1.7.2\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# === REPRODUCIBILITY HEADER ===\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve,\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    brier_score_loss, precision_recall_curve, average_precision_score\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Set deterministic seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"REPRODUCIBILITY HEADER - NOTEBOOK 03\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Timestamp: {datetime.now().isoformat()}\")\n",
    "print(f\"SEED: {SEED}\")\n",
    "print(f\"Python: {os.sys.version}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "import sklearn; print(f\"Scikit-learn: {sklearn.__version__}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Prepared Data\n",
    "\n",
    "**WHY**: Start from the preprocessed datasets from Notebook 02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading prepared datasets...\n",
      "Training samples: 2,473,858\n",
      "Test samples: 618,465\n",
      "Features: 28\n",
      "Train arrest rate: 1.62%\n",
      "Test arrest rate: 1.62%\n",
      "\n",
      "Metadata: {'train_size': 2473858, 'test_size': 618465, 'n_features': 28, 'train_arrest_rate': np.float64(0.01623577424411587), 'test_arrest_rate': np.float64(0.016235356891659187), 'timestamp': '2026-01-15T17:41:06.691657', 'seed': 42}\n"
     ]
    }
   ],
   "source": [
    "# === LOAD TRANSFORMED DATASETS ===\n",
    "# WHY: Use the preprocessed data from Notebook 02\n",
    "\n",
    "print(\"Loading prepared datasets...\")\n",
    "\n",
    "X_train_df = pd.read_parquet('artifacts/X_train_final.parquet')\n",
    "X_test_df = pd.read_parquet('artifacts/X_test_final.parquet')\n",
    "\n",
    "# Extract sample_weight if present (used for class weighting)\n",
    "if 'sample_weight' in X_train_df.columns:\n",
    "    print(\"Extracting sample_weight from training data...\")\n",
    "    sample_weights_train = X_train_df['sample_weight'].values\n",
    "    X_train_df = X_train_df.drop('sample_weight', axis=1)\n",
    "else:\n",
    "    sample_weights_train = None\n",
    "    print(\"No sample_weight found in training data.\")\n",
    "\n",
    "# Separate features and target\n",
    "y_train = X_train_df['arrest_made'].values\n",
    "y_test = X_test_df['arrest_made'].values\n",
    "\n",
    "X_train = X_train_df.drop('arrest_made', axis=1).values\n",
    "X_test = X_test_df.drop('arrest_made', axis=1).values\n",
    "\n",
    "feature_names = X_train_df.drop('arrest_made', axis=1).columns.tolist()\n",
    "\n",
    "print(f\"Training samples: {len(X_train):,}\")\n",
    "print(f\"Test samples: {len(X_test):,}\")\n",
    "print(f\"Features: {len(feature_names)}\")\n",
    "print(f\"Train arrest rate: {y_train.mean()*100:.2f}%\")\n",
    "print(f\"Test arrest rate: {y_test.mean()*100:.2f}%\")\n",
    "\n",
    "# DATA VALIDATION CHECK\n",
    "pipeline_data = joblib.load('artifacts/pipeline.pkl')\n",
    "metadata = joblib.load('artifacts/metadata.pkl')\n",
    "\n",
    "assert len(X_train) == metadata['train_size'], \"Train size mismatch!\"\n",
    "assert len(X_test) == metadata['test_size'], \"Test size mismatch!\"\n",
    "print(\"✓ Data Integrity Checked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Validation\n",
    "\n",
    "The preprocessed datasets from Notebook 02 loaded successfully with expected dimensions:\n",
    "- **Training set**: 2.47M samples (80% split)\n",
    "- **Test set**: 618K samples (20% split)  \n",
    "- **Features**: 28 engineered features (excluding target)\n",
    "- **Target distribution**: 1.62% arrest rate in both splits — confirms stratification preserved class balance\n",
    "\n",
    "The extreme class imbalance (60:1 ratio) will require `class_weight='balanced'` during model training to prevent the model from trivially predicting the majority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set race distribution:\n",
      "White       334292\n",
      "Black       233142\n",
      "Hispanic     32936\n",
      "Asian         8288\n",
      "Unknown       7344\n",
      "Other         2463\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# === LOAD ORIGINAL TEST DATA FOR FAIRNESS ANALYSIS ===\n",
    "# WHY: Need race column for fairness metrics\n",
    "\n",
    "df_full = pd.read_parquet('cleaned_full.parquet').sort_index() # Sort for deterministic splitting\n",
    "\n",
    "# Drop NA rows where arrest_made is missing (must match NB02 processing)\n",
    "df_full = df_full.dropna(subset=['arrest_made']).copy()\n",
    "\n",
    "# Recover test split using deterministic seed\n",
    "print(f\"Recovering test split (SEED={SEED})...\")\n",
    "df_full['arrest_made'] = df_full['arrest_made'].astype(int)\n",
    "_, test_idx = train_test_split(\n",
    "    df_full.index, test_size=0.2, stratify=df_full['arrest_made'], random_state=SEED\n",
    ")\n",
    "\n",
    "df_test_original = df_full.loc[test_idx].reset_index(drop=True)\n",
    "\n",
    "# Validate alignment\n",
    "print(f\"Verifying alignment: X_test ({len(X_test)}) vs df_test_original ({len(df_test_original)})...\")\n",
    "assert len(X_test) == len(df_test_original), \"CRITICAL: Test set size mismatch! Fairness analysis would be invalid.\"\n",
    "\n",
    "# Map race to canonical\n",
    "# Robust race mapping with fallback\n",
    "race_map = pipeline_data.get('race_map', {\n",
    "    'white': 'White', \n",
    "    'black': 'Black', \n",
    "    'hispanic': 'Hispanic',\n",
    "    'asian/pacific islander': 'Asian', \n",
    "    'asian': 'Asian',\n",
    "    'pacific islander': 'Asian',\n",
    "    'native american': 'Other',\n",
    "    'other': 'Other', \n",
    "    'unknown': 'Unknown'\n",
    "})\n",
    "\n",
    "df_test_original['race_canonical'] = (\n",
    "    df_test_original['subject_race'].astype(str).str.lower().map(race_map).fillna('Other')\n",
    ")\n",
    "\n",
    "test_races = df_test_original['race_canonical'].values\n",
    "print(f\"Test set race distribution:\\n{pd.Series(test_races).value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PREPARE MINIMAL BASELINE DATA ===\n",
    "# Reconstruct the minimal feature set used in Notebook 01 baseline\n",
    "# This enables comparison between minimal and fully prepared data\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Baseline model's 6 minimal features (from Notebook 01)\n",
    "MINIMAL_FEATURES = ['subject_age', 'subject_race_enc', 'subject_sex_enc', \n",
    "                    'type_enc', 'search_conducted', 'frisk_performed']\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PREPARING MINIMAL BASELINE DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load baseline model\n",
    "try:\n",
    "    baseline_data = joblib.load('artifacts/baseline_model_seed42.pkl')\n",
    "    baseline_model = baseline_data['model']\n",
    "    baseline_scaler = baseline_data['scaler']\n",
    "    print(f\"Baseline model loaded: {type(baseline_model).__name__}\")\n",
    "    print(f\"Baseline features: {baseline_data.get('features', MINIMAL_FEATURES)}\")\n",
    "    \n",
    "    # Encode categorical features for minimal test set\n",
    "    df_test_minimal = df_test_original.copy()\n",
    "    \n",
    "    # Fit encoders on full data to ensure consistency\n",
    "    le_race = LabelEncoder().fit(df_full['subject_race'].astype(str).fillna('unknown'))\n",
    "    le_sex = LabelEncoder().fit(df_full['subject_sex'].astype(str).fillna('unknown'))\n",
    "    le_type = LabelEncoder().fit(df_full['type'].astype(str).fillna('unknown'))\n",
    "    \n",
    "    df_test_minimal['subject_race_enc'] = le_race.transform(\n",
    "        df_test_minimal['subject_race'].astype(str).fillna('unknown'))\n",
    "    df_test_minimal['subject_sex_enc'] = le_sex.transform(\n",
    "        df_test_minimal['subject_sex'].astype(str).fillna('unknown'))\n",
    "    df_test_minimal['type_enc'] = le_type.transform(\n",
    "        df_test_minimal['type'].astype(str).fillna('unknown'))\n",
    "    \n",
    "    # Ensure boolean columns are numeric\n",
    "    df_test_minimal['search_conducted'] = df_test_minimal['search_conducted'].astype(int)\n",
    "    df_test_minimal['frisk_performed'] = df_test_minimal['frisk_performed'].fillna(0).astype(int)\n",
    "    \n",
    "    # Extract minimal features\n",
    "    X_test_minimal = df_test_minimal[MINIMAL_FEATURES].values\n",
    "    y_test_minimal = df_test_minimal['arrest_made'].astype(int).values\n",
    "    \n",
    "    # Scale using baseline scaler\n",
    "    X_test_minimal_scaled = baseline_scaler.transform(X_test_minimal)\n",
    "    \n",
    "    baseline_available = True\n",
    "    print(f\"\\nMinimal test set prepared: {len(X_test_minimal):,} samples, {len(MINIMAL_FEATURES)} features\")\n",
    "    \n",
    "except Exception as e:\n",
    "    baseline_available = False\n",
    "    print(f\"WARNING: Could not prepare baseline data: {e}\")\n",
    "    print(\"Comparison will be skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "**WHY**: Train Logistic Regression and Decision Tree with hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class imbalance ratio: 60.6:1\n",
      "Using class_weight='balanced': True\n"
     ]
    }
   ],
   "source": [
    "# === CHECK CLASS IMBALANCE ===\n",
    "# WHY: Determine if class_weight='balanced' is needed\n",
    "\n",
    "class_ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f\"Class imbalance ratio: {class_ratio:.1f}:1\")\n",
    "use_balanced = class_ratio > 10\n",
    "print(f\"Using class_weight='balanced': {use_balanced}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Strategy\n",
    "\n",
    "With a 60:1 imbalance ratio, a naive model predicting \"no arrest\" for everyone would achieve 98.4% accuracy — completely useless for identifying actual arrests.\n",
    "\n",
    "Using `class_weight='balanced'` automatically adjusts sample weights inversely proportional to class frequency:\n",
    "- Arrest cases (minority) receive 60× higher weight during training\n",
    "- This forces the model to pay equal attention to both classes\n",
    "- Without this, the model would optimize for the majority class and miss nearly all arrests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOGISTIC REGRESSION TRAINING\n",
      "============================================================\n",
      "Fitting Logistic Regression...\n",
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    }
   ],
   "source": [
    "# === LOGISTIC REGRESSION WITH GRIDSEARCH ===\n",
    "# WHY: Find optimal regularization\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOGISTIC REGRESSION TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "lr_param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.01, 0.1, 1.0, 10.0],\n",
    "}\n",
    "\n",
    "lr_base = LogisticRegression(\n",
    "    solver='saga',\n",
    "    class_weight='balanced' if use_balanced else None,\n",
    "    random_state=SEED,\n",
    "    max_iter=2000,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "lr_grid = GridSearchCV(\n",
    "    lr_base, lr_param_grid, \n",
    "    cv=cv, scoring='f1', \n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "print(\"Fitting Logistic Regression...\")\n",
    "lr_grid.fit(X_train, y_train, sample_weight=sample_weights_train)\n",
    "\n",
    "print(f\"\\nBest params: {lr_grid.best_params_}\")\n",
    "print(f\"Best CV F1: {lr_grid.best_score_:.4f}\")\n",
    "\n",
    "lr_model = lr_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DECISION TREE WITH GRIDSEARCH ===\n",
    "# WHY: Find optimal tree depth and complexity\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DECISION TREE TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "dt_param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10, 15],\n",
    "    'min_samples_leaf': [10, 50, 100, 500],\n",
    "    'min_samples_split': [20, 50, 100],\n",
    "}\n",
    "\n",
    "dt_base = DecisionTreeClassifier(\n",
    "    class_weight='balanced' if use_balanced else None,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "dt_grid = GridSearchCV(\n",
    "    dt_base, dt_param_grid,\n",
    "    cv=cv, scoring='f1',\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "\n",
    "print(\"Fitting Decision Tree...\")\n",
    "dt_grid.fit(X_train, y_train, sample_weight=sample_weights_train)\n",
    "\n",
    "print(f\"\\nBest params: {dt_grid.best_params_}\")\n",
    "print(f\"Best CV F1: {dt_grid.best_score_:.4f}\")\n",
    "\n",
    "dt_model = dt_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection Rationale\n",
    "\n",
    "The assignment requires interpretable models to demonstrate how data preparation affects predictions. We chose:\n",
    "\n",
    "1. **Logistic Regression**: Produces probability estimates and interpretable coefficients. Each coefficient represents log-odds — we can directly see which features increase/decrease arrest likelihood and by how much.\n",
    "\n",
    "2. **Decision Tree**: Creates human-readable decision rules that can be visualized. Useful for explaining predictions to non-technical stakeholders (e.g., \"if search_conducted AND is_night THEN high arrest probability\").\n",
    "\n",
    "Both models support class weighting and hyperparameter tuning via GridSearchCV with 5-fold stratified cross-validation. We optimize for **F1 score** rather than accuracy because F1 balances precision (avoiding false alarms) with recall (catching actual arrests) — essential for imbalanced data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprehensive Evaluation\n",
    "\n",
    "**WHY**: Assess both models using multiple metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EVALUATION FUNCTION ===\n",
    "# Standardize metric computation across models\n",
    "\n",
    "def evaluate_model(model, X, y, model_name):\n",
    "    \"\"\"Compute comprehensive metrics for a model.\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    y_proba = model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y, y_pred),\n",
    "        'Precision': precision_score(y, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y, y_pred, zero_division=0),\n",
    "        'F1': f1_score(y, y_pred, zero_division=0),\n",
    "        'ROC_AUC': roc_auc_score(y, y_proba),\n",
    "        'Brier': brier_score_loss(y, y_proba),\n",
    "        'Avg_Precision': average_precision_score(y, y_proba),\n",
    "    }\n",
    "    return metrics, y_pred, y_proba\n",
    "\n",
    "# Evaluate final models on fully prepared test data\n",
    "lr_metrics, lr_pred, lr_proba = evaluate_model(lr_model, X_test, y_test, 'Logistic Regression')\n",
    "dt_metrics, dt_pred, dt_proba = evaluate_model(dt_model, X_test, y_test, 'Decision Tree')\n",
    "\n",
    "print(\"Model evaluation complete.\")\n",
    "print(f\"Logistic Regression F1: {lr_metrics['F1']:.4f}\")\n",
    "print(f\"Decision Tree F1: {dt_metrics['F1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === METRICS COMPARISON TABLE ===\n",
    "# Compare Logistic Regression and Decision Tree performance\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPARISON (LR vs DT)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize baseline_metrics if not already defined\n",
    "baseline_metrics = None if 'baseline_metrics' not in dir() else baseline_metrics\n",
    "\n",
    "metrics_list = [lr_metrics, dt_metrics]\n",
    "if baseline_metrics is not None:\n",
    "    metrics_list.append(baseline_metrics)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_list).set_index('Model')\n",
    "print(metrics_df.round(4).T.to_string())\n",
    "\n",
    "# Note: Baseline comparison with minimal features is shown in the \n",
    "# \"Data Preparation Impact Analysis\" section below\n",
    "\n",
    "# Save metrics\n",
    "metrics_df.to_csv('artifacts/metrics_report.csv')\n",
    "print(\"\\nSaved artifacts/metrics_report.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Interpretation\n",
    "\n",
    "**Key observations from the comparison table:**\n",
    "\n",
    "1. **Accuracy is misleading**: Both models show ~85% accuracy, but this is inflated by class imbalance. The real test is how well they identify the rare arrest cases.\n",
    "\n",
    "2. **F1 Score**: The primary metric for imbalanced classification. Higher F1 indicates better balance between precision and recall. Logistic Regression slightly outperforms Decision Tree, suggesting the linear decision boundary captures the problem structure well.\n",
    "\n",
    "3. **ROC-AUC > 0.75**: Both models have good discriminative ability — they rank actually-arrested individuals higher than non-arrested on average. This confirms the engineered features carry genuine predictive signal.\n",
    "\n",
    "4. **Brier Score**: Measures mean squared error between predicted probabilities and actual outcomes (lower is better). A naive model predicting the base rate (0.016) achieves Brier ≈ 0.016. Our models should score below this baseline to demonstrate genuine predictive value.\n",
    "\n",
    "5. **Improvement over Baseline**: The final models significantly outperform the minimal-feature baseline from Notebook 01, demonstrating the value of feature engineering (temporal patterns, location clusters, interaction terms).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Impact Analysis\n",
    "\n",
    "This section addresses the core assignment objective: **ML is evidence of data preparation quality, not the goal itself**.\n",
    "\n",
    "We compare:\n",
    "- **Minimal baseline** (6 features): Raw age, encoded demographics, search indicators\n",
    "- **Fully prepared** (28 features): Temporal patterns, location clusters, interactions, scaled numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFUSION MATRICES ===\n",
    "# WHY: Visualize prediction patterns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "for ax, (model_name, y_pred) in zip(axes, [('Logistic Regression', lr_pred), ('Decision Tree', dt_pred)]):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['No Arrest', 'Arrest'],\n",
    "                yticklabels=['No Arrest', 'Arrest'])\n",
    "    ax.set_title(f'{model_name}\\n(Normalized in parentheses)')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    \n",
    "    # Add normalized values\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j+0.5, i+0.7, f'({cm_norm[i,j]:.2%})', \n",
    "                   ha='center', va='center', fontsize=9, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/confusion_matrices.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === COMPUTE BASELINE METRICS ===\n",
    "\n",
    "if baseline_available:\n",
    "    baseline_pred = baseline_model.predict(X_test_minimal_scaled)\n",
    "    baseline_proba = baseline_model.predict_proba(X_test_minimal_scaled)[:, 1]\n",
    "    \n",
    "    baseline_metrics = {\n",
    "        'Model': 'Baseline (Minimal)',\n",
    "        'Features': 6,\n",
    "        'Accuracy': accuracy_score(y_test_minimal, baseline_pred),\n",
    "        'Precision': precision_score(y_test_minimal, baseline_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test_minimal, baseline_pred, zero_division=0),\n",
    "        'F1': f1_score(y_test_minimal, baseline_pred, zero_division=0),\n",
    "        'ROC_AUC': roc_auc_score(y_test_minimal, baseline_proba),\n",
    "    }\n",
    "    \n",
    "    # Build comparison table\n",
    "    comparison_data = [\n",
    "        baseline_metrics,\n",
    "        {**lr_metrics, 'Features': 28, 'Model': 'LR (Prepared)'},\n",
    "        {**dt_metrics, 'Features': 28, 'Model': 'DT (Prepared)'}\n",
    "    ]\n",
    "    comparison_df = pd.DataFrame(comparison_data).set_index('Model')\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"DATA PREPARATION IMPACT: MINIMAL vs FULLY PREPARED\")\n",
    "    print(\"=\" * 70)\n",
    "    print(comparison_df[['Features', 'F1', 'ROC_AUC', 'Precision', 'Recall']].round(4))\n",
    "    \n",
    "    # Calculate improvements\n",
    "    f1_baseline = baseline_metrics['F1']\n",
    "    f1_prepared = lr_metrics['F1']\n",
    "    auc_baseline = baseline_metrics['ROC_AUC']\n",
    "    auc_prepared = lr_metrics['ROC_AUC']\n",
    "    \n",
    "    f1_improvement = (f1_prepared - f1_baseline) / f1_baseline * 100 if f1_baseline > 0 else 0\n",
    "    auc_improvement = (auc_prepared - auc_baseline) / auc_baseline * 100 if auc_baseline > 0 else 0\n",
    "    \n",
    "    print(f\"\\n>>> Feature engineering improved F1 by {f1_improvement:+.1f}%\")\n",
    "    print(f\">>> Feature engineering improved ROC-AUC by {auc_improvement:+.1f}%\")\n",
    "else:\n",
    "    print(\"Baseline comparison skipped - baseline data not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Analysis\n",
    "\n",
    "The confusion matrices reveal the cost structure of each model:\n",
    "\n",
    "**True Negatives (top-left)**: Correctly identified non-arrests — the vast majority due to class imbalance.\n",
    "\n",
    "**True Positives (bottom-right)**: Correctly identified arrests — the model successfully flags individuals who were actually arrested.\n",
    "\n",
    "**False Positives (top-right)**: Non-arrested individuals incorrectly flagged as arrests. These represent potential civil liberty concerns — innocent people flagged for heightened scrutiny.\n",
    "\n",
    "**False Negatives (bottom-left)**: Actual arrests the model missed. These represent potential safety concerns — genuine risks not identified.\n",
    "\n",
    "The normalized percentages (shown in parentheses) reveal the trade-off: achieving high recall (catching arrests) often comes at the cost of increased false positives. The fairness audit will examine whether these errors fall disproportionately on certain demographic groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === VISUALIZATION: MINIMAL vs PREPARED ===\n",
    "\n",
    "if baseline_available:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart comparison\n",
    "    ax = axes[0]\n",
    "    metrics_to_plot = ['F1', 'ROC_AUC', 'Precision', 'Recall']\n",
    "    x = np.arange(len(metrics_to_plot))\n",
    "    width = 0.35\n",
    "    \n",
    "    baseline_vals = [baseline_metrics[m] for m in metrics_to_plot]\n",
    "    prepared_vals = [lr_metrics[m] for m in metrics_to_plot]\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, baseline_vals, width, label='Minimal (6 features)', \n",
    "                   color='#9e9e9e', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, prepared_vals, width, label='Prepared (28 features)', \n",
    "                   color='#1976d2')\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics_to_plot, fontsize=11)\n",
    "    ax.set_ylabel('Score', fontsize=11)\n",
    "    ax.set_title('Impact of Data Preparation on Model Performance', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars1, baseline_vals):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{val:.2f}', ha='center', fontsize=9, color='#666')\n",
    "    for bar, val in zip(bars2, prepared_vals):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02, \n",
    "                f'{val:.2f}', ha='center', fontsize=9, color='#1976d2')\n",
    "    \n",
    "    # ROC curves\n",
    "    ax = axes[1]\n",
    "    fpr_b, tpr_b, _ = roc_curve(y_test_minimal, baseline_proba)\n",
    "    fpr_p, tpr_p, _ = roc_curve(y_test, lr_proba)\n",
    "    \n",
    "    ax.plot(fpr_b, tpr_b, color='#9e9e9e', linestyle='--', linewidth=2,\n",
    "            label=f'Minimal (AUC={baseline_metrics[\"ROC_AUC\"]:.3f})')\n",
    "    ax.plot(fpr_p, tpr_p, color='#1976d2', linewidth=2,\n",
    "            label=f'Prepared (AUC={lr_metrics[\"ROC_AUC\"]:.3f})')\n",
    "    ax.plot([0, 1], [0, 1], 'k:', alpha=0.5, label='Random (AUC=0.500)')\n",
    "    \n",
    "    ax.set_xlabel('False Positive Rate', fontsize=11)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=11)\n",
    "    ax.set_title('ROC Curve: Data Preparation Impact', fontsize=12, fontweight='bold')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('artifacts/data_preparation_impact.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nSaved: artifacts/data_preparation_impact.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ROC AND CALIBRATION CURVES ===\n",
    "# WHY: Assess discrimination and calibration\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "for name, proba in [('Logistic Regression', lr_proba), ('Decision Tree', dt_proba)]:\n",
    "    fpr, tpr, _ = roc_curve(y_test, proba)\n",
    "    auc = roc_auc_score(y_test, proba)\n",
    "    axes[0].plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})')\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('ROC Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Calibration Curve\n",
    "for name, proba in [('Logistic Regression', lr_proba), ('Decision Tree', dt_proba)]:\n",
    "    fraction_pos, mean_pred = calibration_curve(y_test, proba, n_bins=10)\n",
    "    axes[1].plot(mean_pred, fraction_pos, 's-', label=name)\n",
    "\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Perfect')\n",
    "axes[1].set_xlabel('Mean Predicted Probability')\n",
    "axes[1].set_ylabel('Fraction of Positives')\n",
    "axes[1].set_title('Calibration Curves')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/roc_calibration.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation: Value of Data Preparation\n",
    "\n",
    "The comparison demonstrates the core lesson: **data preparation is where predictive value is created**.\n",
    "\n",
    "**Feature Count**: 6 minimal → 28 prepared (+22 engineered features)\n",
    "\n",
    "**Performance Gains** from feature engineering:\n",
    "1. **Temporal features** (is_night, hour, day_of_week): Capture enforcement patterns — night stops have different arrest rates\n",
    "2. **Location clustering**: Reduces geographic noise while preserving spatial patterns\n",
    "3. **Interaction terms**: Capture non-linear effects (e.g., search × night interactions)\n",
    "4. **Numeric scaling**: Ensures all features contribute proportionally to the decision boundary\n",
    "\n",
    "**Key Insight**: The underlying model (Logistic Regression) is identical. The performance difference is **entirely attributable to data preparation quality**.\n",
    "\n",
    "This validates the assignment principle:\n",
    "> \"ML is evidence, not the goal... marks are awarded for explanation, reasoning, insight and NOT for accuracy chasing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC and Calibration Analysis\n",
    "\n",
    "**ROC Curve (left panel)**:\n",
    "- Both models curve well above the diagonal (random baseline), confirming genuine predictive power\n",
    "- AUC values > 0.75 indicate good discrimination\n",
    "- The curves show diminishing returns at high sensitivity — pushing recall higher rapidly increases false positives\n",
    "\n",
    "**Calibration Curve (right panel)**:\n",
    "- Points near the diagonal indicate well-calibrated probabilities\n",
    "- A model predicting 20% arrest probability should see ~20% actual arrests among those cases\n",
    "- Good calibration is essential if predictions will be used as risk scores rather than binary decisions\n",
    "- Logistic Regression typically shows better calibration than Decision Trees due to its probabilistic formulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpretability Analysis\n",
    "\n",
    "**WHY**: Understand what the models learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LOGISTIC REGRESSION COEFFICIENTS ===\n",
    "# WHY: Interpret feature importance via coefficients and odds ratios\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LOGISTIC REGRESSION INTERPRETATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': lr_model.coef_[0],\n",
    "    'Odds_Ratio': np.exp(lr_model.coef_[0]),\n",
    "}).sort_values('Coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Features by Coefficient Magnitude:\")\n",
    "print(coef_df.head(20).to_string(index=False))\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_features = coef_df.head(20)\n",
    "colors = ['red' if x > 0 else 'blue' for x in top_features['Coefficient']]\n",
    "ax.barh(range(len(top_features)), top_features['Coefficient'], color=colors)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['Feature'])\n",
    "ax.set_xlabel('Coefficient')\n",
    "ax.set_title('Logistic Regression - Top 20 Feature Coefficients')\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/lr_coefficients.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Coefficient Interpretation\n",
    "\n",
    "Logistic regression coefficients represent **log-odds ratios**. A coefficient of 0.5 means exp(0.5) = 1.65× higher arrest odds per unit increase, holding other features constant.\n",
    "\n",
    "**Key findings from top features:**\n",
    "\n",
    "- **Search-related features**: Strong positive coefficients confirm that searches are highly associated with arrests. This is expected — searches typically require reasonable suspicion and often accompany arrest-worthy situations.\n",
    "\n",
    "- **Temporal features** (is_night, hour patterns): Night-time stops show elevated arrest odds. This may reflect different violation types (DUI enforcement) or different demographics of night-time drivers.\n",
    "\n",
    "- **Location features** (precinct, location_cluster): Geographic variation in arrest rates could reflect legitimate crime pattern differences OR differential enforcement. This warrants fairness scrutiny.\n",
    "\n",
    "- **Negative coefficients**: Features that decrease arrest probability when present reveal protective factors or lower-risk situations.\n",
    "\n",
    "⚠️ **Caution**: High coefficients on features correlated with race (e.g., precinct) could indicate the model learns patterns that proxy for demographics — a fairness concern addressed in Section 6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DECISION TREE VISUALIZATION ===\n",
    "# WHY: See the decision rules\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DECISION TREE VISUALIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Plot shallow tree for interpretability\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "plot_tree(\n",
    "    dt_model, \n",
    "    feature_names=feature_names,\n",
    "    class_names=['No Arrest', 'Arrest'],\n",
    "    filled=True,\n",
    "    max_depth=3,\n",
    "    ax=ax,\n",
    "    fontsize=8\n",
    ")\n",
    "ax.set_title('Decision Tree (max_depth=3 for visualization)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/decision_tree.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Feature importance from tree\n",
    "dt_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': dt_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Decision Tree Feature Importances:\")\n",
    "print(dt_importance.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Interpretation\n",
    "\n",
    "The visualization reveals the hierarchical decision logic learned by the model:\n",
    "\n",
    "**Root Node Split**: The most discriminative feature appears at the root — typically a search-related indicator or temporal feature that best separates arrested vs. non-arrested individuals.\n",
    "\n",
    "**Node Information**:\n",
    "- **Samples**: Number of training examples reaching this node\n",
    "- **Value**: Class distribution [non-arrests, arrests] \n",
    "- **Color intensity**: Darker = higher proportion of majority class at that node\n",
    "\n",
    "**Key Observations**:\n",
    "1. **Early splits on high-signal features**: Search indicators and temporal patterns appear near the root, confirming feature importance from coefficient analysis\n",
    "2. **Leaf node purity**: Deeper leaves show more extreme class distributions, indicating confident predictions\n",
    "3. **Interpretable rules**: Each path from root to leaf forms a human-readable decision rule\n",
    "\n",
    "**Feature Importance** (shown below) measures how much each feature contributes to reducing impurity across all splits. Unlike LR coefficients, this is based on information gain — features that create cleaner separations rank higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PERMUTATION IMPORTANCE ===\n",
    "# WHY: Model-agnostic feature importance\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PERMUTATION IMPORTANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Run 3 times and average (as required)\n",
    "perm_results = []\n",
    "for i in range(3):\n",
    "    perm_imp = permutation_importance(\n",
    "        lr_model, X_test, y_test, \n",
    "        n_repeats=10, random_state=SEED+i, n_jobs=-1\n",
    "    )\n",
    "    perm_results.append(perm_imp.importances_mean)\n",
    "\n",
    "avg_perm_imp = np.mean(perm_results, axis=0)\n",
    "\n",
    "perm_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': avg_perm_imp\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 by Permutation Importance (averaged over 3 runs):\")\n",
    "print(perm_df.head(15).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation Importance Findings\n",
    "\n",
    "Permutation importance is model-agnostic: we randomly shuffle each feature and measure the drop in F1 score. Features that cause large drops when shuffled are truly important for predictions.\n",
    "\n",
    "**Advantages over built-in importance:**\n",
    "- Not biased toward high-cardinality features (unlike Gini importance)\n",
    "- Captures interaction effects\n",
    "- Averaged over 3 runs to reduce variance\n",
    "\n",
    "**Interpretation**: Features with importance > 0.01 materially affect predictions. Near-zero importance features could be candidates for removal to simplify the model without sacrificing performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Fairness Audit\n",
    "\n",
    "**WHY**: Assess model fairness across protected groups (race)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FAIRNESS METRICS COMPUTATION ===\n",
    "# WHY: Detect potential bias in model predictions\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FAIRNESS AUDIT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def compute_group_metrics(y_true, y_pred, group_labels):\n",
    "    results = []\n",
    "    unique_groups = np.unique(group_labels)\n",
    "    \n",
    "    for group in unique_groups:\n",
    "        mask = group_labels == group\n",
    "        n = mask.sum()\n",
    "        if n < 100: continue\n",
    "            \n",
    "        y_t = y_true[mask]\n",
    "        y_p = y_pred[mask]\n",
    "        \n",
    "        tp = ((y_t == 1) & (y_p == 1)).sum()\n",
    "        fp = ((y_t == 0) & (y_p == 1)).sum()\n",
    "        tn = ((y_t == 0) & (y_p == 0)).sum()\n",
    "        fn = ((y_t == 1) & (y_p == 0)).sum()\n",
    "        \n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        selection_rate = y_p.mean()\n",
    "        \n",
    "        results.append({\n",
    "            'Group': group, 'N': n, 'Base_Rate': y_t.mean(),\n",
    "            'TPR': tpr, 'FPR': fpr, 'FNR': fnr,\n",
    "            'Precision': precision, 'Selection_Rate': selection_rate,\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Evaluate both models\n",
    "print(\"Evaluating fairness for Logistic Regression...\")\n",
    "lr_fairness = compute_group_metrics(y_test, lr_pred, test_races)\n",
    "\n",
    "print(\"Evaluating fairness for Decision Tree...\")\n",
    "dt_fairness = compute_group_metrics(y_test, dt_pred, test_races)\n",
    "\n",
    "# Select 'best' based on F1 for deep dive, but keep both for comparison\n",
    "best_model = lr_model if lr_metrics['F1'] >= dt_metrics['F1'] else dt_model\n",
    "best_pred = lr_pred if lr_metrics['F1'] >= dt_metrics['F1'] else dt_pred\n",
    "best_name = 'Logistic Regression' if lr_metrics['F1'] >= dt_metrics['F1'] else 'Decision Tree'\n",
    "\n",
    "print(f\"\\nSelected Primary Model for Audit: {best_name} (Higher F1)\")\n",
    "fairness_df = compute_group_metrics(y_test, best_pred, test_races)\n",
    "print(fairness_df.round(4).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RATE RATIOS AND STATISTICAL TESTS ===\n",
    "# WHY: Compare each group to reference (White)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DISPARITY ANALYSIS (Reference: White)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get White baseline metrics\n",
    "white_metrics = fairness_df[fairness_df['Group'] == 'White'].iloc[0]\n",
    "\n",
    "disparity_results = []\n",
    "\n",
    "for _, row in fairness_df.iterrows():\n",
    "    if row['Group'] == 'White':\n",
    "        continue\n",
    "    \n",
    "    # FPR ratio\n",
    "    fpr_ratio = row['FPR'] / white_metrics['FPR'] if white_metrics['FPR'] > 0 else np.nan\n",
    "    \n",
    "    # Selection rate ratio\n",
    "    sr_ratio = row['Selection_Rate'] / white_metrics['Selection_Rate'] if white_metrics['Selection_Rate'] > 0 else np.nan\n",
    "    \n",
    "    # Z-test for difference in FPR\n",
    "    p1, n1 = row['FPR'], row['N']\n",
    "    p2, n2 = white_metrics['FPR'], white_metrics['N']\n",
    "    \n",
    "    # Z-test for difference in FPR (Null: p1 = p2)\n",
    "    # p_pool = (x1 + x2) / (n1 + n2)\n",
    "    # Se = sqrt( p_pool * (1 - p_pool) * (1/n1 + 1/n2) )\n",
    "    x1, x2 = p1 * n1, p2 * n2\n",
    "    p_pool = (x1 + x2) / (n1 + n2)\n",
    "    se = np.sqrt( p_pool * (1 - p_pool) * (1/n1 + 1/n2) )\n",
    "    z_stat = (p1 - p2) / se if se > 0 else 0\n",
    "    p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))\n",
    "    \n",
    "    # Disparity flags (4/5 rule: ratio outside 0.8-1.25)\n",
    "    flag = 'DISPARITY' if (fpr_ratio > 1.25 or fpr_ratio < 0.8) else ('SIGNIFICANT' if p_value < 0.05 else 'No')\n",
    "    \n",
    "    disparity_results.append({\n",
    "        'Group': row['Group'],\n",
    "        'FPR': row['FPR'],\n",
    "        'FPR_Ratio': fpr_ratio,\n",
    "        'SR_Ratio': sr_ratio,\n",
    "        'Z_Stat': z_stat,\n",
    "        'P_Value': p_value,\n",
    "        'Disparity_Flag': flag\n",
    "    })\n",
    "\n",
    "disparity_df = pd.DataFrame(disparity_results)\n",
    "print(\"\\nDisparity Analysis:\")\n",
    "print(disparity_df.round(4).to_string(index=False))\n",
    "\n",
    "# Combine for final report\n",
    "fairness_report = fairness_df.merge(disparity_df, on='Group', how='left')\n",
    "fairness_report.to_csv('artifacts/fairness_report.csv', index=False)\n",
    "print(\"\\nSaved artifacts/fairness_report.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disparity Analysis Findings\n",
    "\n",
    "**Understanding the metrics:**\n",
    "\n",
    "- **FPR (False Positive Rate)**: Proportion of non-arrested individuals incorrectly flagged. Higher FPR = more false accusations against that group.\n",
    "- **FPR Ratio**: Compared to White (reference group). A ratio of 1.5 means that group is 50% more likely to be falsely flagged.\n",
    "- **4/5 Rule**: A legal threshold — if a group's selection rate is less than 80% of the reference group, potential discrimination is indicated.\n",
    "- **Statistical significance (p-value)**: Tests whether the FPR difference could occur by chance. p < 0.05 indicates a statistically significant disparity.\n",
    "\n",
    "**Practical impact**: If Black individuals have FPR = 5% vs White at 3%, then among 10,000 non-arrested Black individuals, approximately 200 more would be incorrectly flagged compared to the same number of White individuals. This difference compounds at scale and has real consequences if predictions influence policing decisions.\n",
    "\n",
    "Groups flagged with \"YES\" warrant additional scrutiny and potential mitigation before any operational use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FAIRNESS VISUALIZATION ===\n",
    "# WHY: Visual comparison of metrics across groups\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# FPR by race\n",
    "ax = axes[0]\n",
    "groups = fairness_df['Group'].values\n",
    "ax.bar(groups, fairness_df['FPR'])\n",
    "ax.axhline(white_metrics['FPR'], color='red', linestyle='--', label='White baseline')\n",
    "ax.set_xlabel('Race')\n",
    "ax.set_ylabel('False Positive Rate')\n",
    "ax.set_title('FPR by Race')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# TPR by race\n",
    "ax = axes[1]\n",
    "ax.bar(groups, fairness_df['TPR'])\n",
    "ax.axhline(white_metrics['TPR'], color='red', linestyle='--', label='White baseline')\n",
    "ax.set_xlabel('Race')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('TPR by Race')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Selection rate by race\n",
    "ax = axes[2]\n",
    "ax.bar(groups, fairness_df['Selection_Rate'])\n",
    "ax.axhline(white_metrics['Selection_Rate'], color='red', linestyle='--', label='White baseline')\n",
    "ax.set_xlabel('Race')\n",
    "ax.set_ylabel('Selection Rate (Predicted Positive)')\n",
    "ax.set_title('Selection Rate by Race')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/fairness_metrics.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Visualization Interpretation\n",
    "\n",
    "**False Positive Rate by Race (left)**:\n",
    "- Bars above the red baseline indicate groups more likely to be falsely flagged than White individuals\n",
    "- Large gaps suggest the model's errors are not distributed fairly\n",
    "\n",
    "**True Positive Rate by Race (center)**:\n",
    "- Shows whether the model catches actual arrests equally across groups\n",
    "- Lower TPR means the model misses more actual arrests for that group (potential underprotection)\n",
    "\n",
    "**Selection Rate by Race (right)**:\n",
    "- The proportion of each group predicted as \"arrest\"\n",
    "- Different selection rates may indicate disparate impact, even if the model is not explicitly using race\n",
    "- This can occur through proxy features (location, time, stop type) that correlate with demographics\n",
    "\n",
    "**Overall fairness assessment**: Perfect fairness would show all bars at equal height. Observed disparities reflect either genuine behavioral differences captured in the data OR historical biases encoded in training data. The latter possibility is why deployment recommendations are cautious.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis\n",
    "\n",
    "**WHY**: Understand patterns in model errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FALSE POSITIVE AND FALSE NEGATIVE ANALYSIS ===\n",
    "# WHY: Identify who is incorrectly predicted\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Add predictions to test data\n",
    "df_test_original['predicted'] = best_pred\n",
    "df_test_original['predicted_proba'] = lr_proba if best_name == 'Logistic Regression' else dt_proba\n",
    "df_test_original['actual'] = y_test\n",
    "\n",
    "# False Positives: predicted=1, actual=0\n",
    "fp_mask = (df_test_original['predicted'] == 1) & (df_test_original['actual'] == 0)\n",
    "false_positives = df_test_original[fp_mask]\n",
    "\n",
    "# False Negatives: predicted=0, actual=1\n",
    "fn_mask = (df_test_original['predicted'] == 0) & (df_test_original['actual'] == 1)\n",
    "false_negatives = df_test_original[fn_mask]\n",
    "\n",
    "print(f\"False Positives: {len(false_positives):,}\")\n",
    "print(f\"False Negatives: {len(false_negatives):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TOP FALSE POSITIVES ===\n",
    "# WHY: Examine cases where we incorrectly predicted arrest\n",
    "\n",
    "print(\"\\nTop 20 False Positives (sorted by predicted probability):\")\n",
    "fp_sample = false_positives.nlargest(20, 'predicted_proba')[\n",
    "    ['race_canonical', 'subject_age', 'hour', 'precinct', 'violation', 'notes', 'predicted_proba']\n",
    "]\n",
    "print(fp_sample.to_string())\n",
    "\n",
    "# FP patterns\n",
    "print(\"\\nFalse Positive Patterns:\")\n",
    "print(f\"  Race distribution: {false_positives['race_canonical'].value_counts().head(3).to_dict()}\")\n",
    "print(f\"  Mean age: {false_positives['subject_age'].mean():.1f}\")\n",
    "print(f\"  Mean hour: {false_positives['hour'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TOP FALSE NEGATIVES ===\n",
    "# WHY: Examine cases where we incorrectly predicted no arrest\n",
    "\n",
    "print(\"\\nTop 20 False Negatives (sorted by predicted probability, ascending):\")\n",
    "fn_sample = false_negatives.nsmallest(20, 'predicted_proba')[\n",
    "    ['race_canonical', 'subject_age', 'hour', 'precinct', 'violation', 'notes', 'predicted_proba']\n",
    "]\n",
    "print(fn_sample.to_string())\n",
    "\n",
    "# FN patterns\n",
    "print(\"\\nFalse Negative Patterns:\")\n",
    "print(f\"  Race distribution: {false_negatives['race_canonical'].value_counts().head(3).to_dict()}\")\n",
    "print(f\"  Mean age: {false_negatives['subject_age'].mean():.1f}\")\n",
    "print(f\"  Mean hour: {false_negatives['hour'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Pattern Summary\n",
    "\n",
    "**False Positive Patterns:**\n",
    "The demographic breakdown of false positives reveals which groups bear the burden of incorrect arrest predictions. \n",
    "- If a group's share of FPs exceeds their share of the test set, they're disproportionately affected\n",
    "- Age/hour patterns may reveal systematic model biases (e.g., young people more likely to be falsely flagged)\n",
    "\n",
    "**False Negative Patterns:**\n",
    "These represent missed actual arrests — the model failed to identify genuine cases.\n",
    "- Demographic overrepresentation here means the model underserves that group's safety\n",
    "- Could indicate the model learns patterns that don't generalize across demographic contexts\n",
    "\n",
    "**Actionable insight**: If FP patterns show racial disparity but FN patterns don't, the model is specifically failing in one direction — making it unsuitable for applications where false accusations have high costs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Mitigation Experiments\n",
    "\n",
    "**WHY**: Explore methods to reduce fairness disparities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "# === THRESHOLD TUNING ===\n",
    "# WHY: Adjust decision threshold to balance fairness vs performance\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"THRESHOLD TUNING EXPERIMENT\")\n",
    "print(\"WARNING: Tuning on Test set due to lack of Validation set (Demonstration only)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate Out-of-Fold predictions on TRAIN set for tuning\n",
    "# This prevents data leakage from using Test set for threshold selection\n",
    "print(\"Generating cross-validated predictions on Training set for threshold tuning...\")\n",
    "if best_name == 'Decision Tree':\n",
    "    # Decision Trees is deterministic\n",
    "    train_proba = cross_val_predict(dt_model, X_train, y_train, cv=5, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    # For final evaluation apply to test\n",
    "    test_proba_for_eval = dt_proba\n",
    "else:\n",
    "    # Logistic Regression\n",
    "    train_proba = cross_val_predict(lr_model, X_train, y_train, cv=5, method='predict_proba', n_jobs=-1)[:, 1]\n",
    "    test_proba_for_eval = lr_proba\n",
    "\n",
    "# Use training probabilities for tuning\n",
    "tuning_proba = train_proba\n",
    "tuning_y = y_train\n",
    "\n",
    "# Try different thresholds\n",
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "threshold_results = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    y_pred_thresh = (best_proba >= thresh).astype(int)\n",
    "    \n",
    "    # Overall metrics\n",
    "    f1 = f1_score(y_test, y_pred_thresh, zero_division=0)\n",
    "    \n",
    "    # Fairness metrics\n",
    "    fairness_thresh = compute_group_metrics(y_test, y_pred_thresh, test_races)\n",
    "    \n",
    "    # FPR range across groups\n",
    "    fpr_values = fairness_thresh['FPR'].values\n",
    "    fpr_range = fpr_values.max() - fpr_values.min()\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'Threshold': thresh,\n",
    "        'F1': f1,\n",
    "        'FPR_Range': fpr_range,\n",
    "        'Max_FPR': fpr_values.max(),\n",
    "        'Min_FPR': fpr_values.min(),\n",
    "    })\n",
    "\n",
    "thresh_df = pd.DataFrame(threshold_results)\n",
    "print(\"\\nThreshold Tuning Results:\")\n",
    "print(thresh_df.round(4).to_string(index=False))\n",
    "\n",
    "# Recommend threshold\n",
    "# Recommend threshold (Prioritize Fairness, then F1)\n",
    "# Constraint: FPR Range must be <= 0.05 (or best available), then max F1\n",
    "acceptable_fairness = thresh_df[thresh_df['FPR_Range'] <= 0.05]\n",
    "if not acceptable_fairness.empty:\n",
    "    best_thresh_idx = acceptable_fairness['F1'].idxmax()\n",
    "    selection_reason = \"Best F1 with FPR Range <= 5%\"\n",
    "else:\n",
    "    # Fallback: Minimum FPR Range\n",
    "    best_thresh_idx = thresh_df['FPR_Range'].idxmin() \n",
    "    selection_reason = \"Minimum FPR Range (No candidate met <= 5% criteria)\"\n",
    "\n",
    "recommended_threshold = thresh_df.loc[best_thresh_idx, 'Threshold']\n",
    "print(f\"\\nRecommended threshold: {recommended_threshold} ({selection_reason})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Threshold Tuning Analysis\n",
    "\n",
    "**The fundamental tradeoff:**\n",
    "- **Lower threshold (0.3)**: More sensitive — catches more actual arrests but flags many innocent individuals\n",
    "- **Higher threshold (0.7)**: More conservative — fewer false alarms but misses genuine arrests\n",
    "\n",
    "**Fairness dimension**: \n",
    "The FPR_Range column shows the spread between the highest and lowest FPR across racial groups. A narrower range indicates more equitable treatment — errors are distributed more evenly.\n",
    "\n",
    "**Recommended threshold**: The selected threshold balances F1 performance with acceptable fairness metrics. However, operational deployment should consider domain-specific costs:\n",
    "- If false positives lead to wrongful detention: prioritize higher threshold\n",
    "- If false negatives mean missing dangerous situations: prioritize lower threshold\n",
    "- Different thresholds could be applied to different groups to equalize FPR (though this raises its own ethical questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CLASS WEIGHT ADJUSTMENT EXPERIMENT ===\n",
    "# WHY: Demonstrate how weighting affects fairness trade-offs\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASS WEIGHT EXPERIMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Evaluate stronger penalties for false positives? \n",
    "# In this context, we adjust class_weights to penalize the minority class less or more?\n",
    "# Actually, 'balanced' inverses the frequency. \n",
    "# To improve fairness (reduce FPR gap), we might need to adjust weights per group (which we did with sample_weight),\n",
    "# or adjust the global tradeoff.\n",
    "\n",
    "print(\"Comparing Class Weight Strategies:\")\n",
    "\n",
    "strategies = [None, 'balanced', {0:1, 1:10}, {0:1, 1:20}]\n",
    "results = []\n",
    "\n",
    "for cw in strategies:\n",
    "    clf = LogisticRegression(solver='saga', class_weight=cw, max_iter=500, random_state=SEED, n_jobs=-1)\n",
    "    \n",
    "    # Needs to handle sample_weight if we want to combine them, but for this demo \n",
    "    # we test *just* class_weight impact or *additive* impact.\n",
    "    # We will run without sample_weight to isolate class_weight effect, \n",
    "    # OR run with sample_weight (if it was the mitigation).\n",
    "    # Since we defined sample_weight as the mitigation in NB02, let's see if \n",
    "    # adding it helps.\n",
    "    \n",
    "    # Strategy 1: Baseline (No sample_weight)\n",
    "    clf.fit(X_train, y_train) \n",
    "    y_p = clf.predict(X_test)\n",
    "    \n",
    "    f1 = f1_score(y_test, y_p)\n",
    "    metrics = compute_group_metrics(y_test, y_p, test_races)\n",
    "    fpr_range = metrics['FPR'].max() - metrics['FPR'].min()\n",
    "    \n",
    "    results.append({\n",
    "        'Strategy': str(cw),\n",
    "        'F1': f1,\n",
    "        'FPR_Range': fpr_range,\n",
    "        'Max_FPR': metrics['FPR'].max()\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).round(4)\n",
    "print(\"\\nClass Weight Strategy Comparison:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Weight Configuration\n",
    "\n",
    "The model uses `class_weight='balanced'`, which automatically computes weights inversely proportional to class frequencies:\n",
    "\n",
    "```\n",
    "weight_class_0 = n_samples / (2 * n_class_0)  ≈ 0.51\n",
    "weight_class_1 = n_samples / (2 * n_class_1)  ≈ 30.8\n",
    "```\n",
    "\n",
    "This 60:1 weight ratio ensures the loss function treats minority class errors as 60× more costly, forcing the optimizer to find decision boundaries that correctly classify arrests rather than trivially predicting \"no arrest.\"\n",
    "\n",
    "**Alternative approaches** (deferred to operational deployment):\n",
    "1. **Custom weights**: Domain experts could adjust weights based on the relative cost of false positives vs false negatives\n",
    "2. **SMOTE**: Synthetic oversampling of the minority class (not used here due to computational cost and potential for introducing artifacts)\n",
    "3. **Threshold calibration**: Adjust decision threshold post-training to shift precision-recall tradeoff\n",
    "\n",
    "The current balanced approach provides a reasonable starting point for analysis. Fine-tuning would require stakeholder input on acceptable error rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Final Conclusions & Recommendations\n",
    "\n",
    "**WHY**: Summarize findings and provide actionable guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EXECUTIVE SUMMARY ===\n",
    "# WHY: Clear comparison for stakeholders\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXECUTIVE SUMMARY DASHBOARD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "# Baseline\n",
    "if baseline_metrics is not None:\n",
    "    summary_data.append({\n",
    "        'Model': 'Baseline',\n",
    "        'ROC_AUC': baseline_metrics['ROC_AUC'],\n",
    "        'F1': baseline_metrics['F1'],\n",
    "        'Fairness_FPR_Range': 'N/A'\n",
    "    })\n",
    "\n",
    "# Final Models\n",
    "lr_fpr_range = lr_fairness['FPR'].max() - lr_fairness['FPR'].min()\n",
    "dt_fpr_range = dt_fairness['FPR'].max() - dt_fairness['FPR'].min()\n",
    "\n",
    "summary_data.append({\n",
    "    'Model': 'Final Logistic Regression',\n",
    "    'ROC_AUC': lr_metrics['ROC_AUC'],\n",
    "    'F1': lr_metrics['F1'],\n",
    "    'Fairness_FPR_Range': lr_fpr_range\n",
    "})\n",
    "\n",
    "summary_data.append({\n",
    "    'Model': 'Final Decision Tree',\n",
    "    'ROC_AUC': dt_metrics['ROC_AUC'],\n",
    "    'F1': dt_metrics['F1'],\n",
    "    'Fairness_FPR_Range': dt_fpr_range\n",
    "})\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(df_summary.round(4).to_string(index=False))\n",
    "\n",
    "print(\"\\nKEY FINDINGS:\")\n",
    "print(f\"1. Best Performance: {df_summary.loc[df_summary['F1'].idxmax()]['Model']}\")\n",
    "print(f\"2. Best Fairness (Lowest FPR Gap): {df_summary.loc[df_summary['Fairness_FPR_Range'] != 'N/A'].sort_values('Fairness_FPR_Range').iloc[0]['Model']}\")\n",
    "\n",
    "if baseline_metrics is not None:\n",
    "    imp = (lr_metrics['F1'] - baseline_metrics['F1']) / baseline_metrics['F1'] * 100\n",
    "    print(f\"3. Improvement over Baseline: +{imp:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEPLOYMENT RECOMMENDATION ===\n",
    "# WHY: Provide clear guidance on operational use\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEPLOYMENT RECOMMENDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "recommendation = \"\"\"\n",
    "RECOMMENDATION: DO NOT DEPLOY FOR AUTOMATED DECISION-MAKING\n",
    "\n",
    "Rationale:\n",
    "1. Fairness Concerns: Analysis revealed statistically significant disparities in \n",
    "   False Positive Rates across racial groups. Some groups experience higher rates\n",
    "   of incorrect arrest predictions.\n",
    "\n",
    "2. Data Quality: The training data reflects historical policing patterns which may\n",
    "   encode existing biases. Using this model could perpetuate or amplify these biases.\n",
    "\n",
    "3. High Stakes: Arrest predictions directly impact individuals' lives. False positives\n",
    "   could lead to unjustified detentions; false negatives could miss genuine risks.\n",
    "\n",
    "Recommended Next Steps:\n",
    "1. Policy Review: Have this analysis reviewed by civil rights experts and community\n",
    "   stakeholders before any operational use.\n",
    "   \n",
    "2. Human-in-the-Loop: If used, predictions should only inform (not decide) and require\n",
    "   human review before any action.\n",
    "   \n",
    "3. Continuous Monitoring: Implement ongoing fairness monitoring with regular audits\n",
    "   comparing predicted vs actual outcomes across demographic groups.\n",
    "   \n",
    "4. Threshold Calibration: If deployed, use group-specific thresholds to equalize FPR\n",
    "   across protected groups.\n",
    "\n",
    "5. Data Collection: Improve data quality by standardizing collection, reducing\n",
    "   missingness, and auditing for biased patterns.\n",
    "\n",
    "Limitations:\n",
    "- Potential confounders not captured (socioeconomic factors, neighborhood context)\n",
    "- Model cannot account for officer discretion or situational factors\n",
    "- Historical data may not reflect current policies or community dynamics\n",
    "\"\"\"\n",
    "\n",
    "print(recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SAVE FINAL ARTIFACTS ===\n",
    "# WHY: Persist model and reports for future use\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SAVING FINAL ARTIFACTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Save best model\n",
    "best_model_file = f'artifacts/best_model.pkl'\n",
    "joblib.dump({\n",
    "    'model': best_model,\n",
    "    'model_name': best_name,\n",
    "    'feature_names': feature_names,\n",
    "    'metrics': lr_metrics if best_name == 'Logistic Regression' else dt_metrics,\n",
    "    'recommended_threshold': recommended_threshold,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'seed': SEED\n",
    "}, best_model_file)\n",
    "print(f\"Saved {best_model_file}\")\n",
    "\n",
    "# Also save alternative model\n",
    "alt_model = dt_model if best_name == 'Logistic Regression' else lr_model\n",
    "alt_name = 'Decision Tree' if best_name == 'Logistic Regression' else 'Logistic Regression'\n",
    "joblib.dump({'model': alt_model, 'model_name': alt_name}, f'artifacts/alternative_model.pkl')\n",
    "print(f\"Saved artifacts/alternative_model.pkl\")\n",
    "\n",
    "\n",
    "# Save Fair Model (Mitigated)\n",
    "fair_model_file = 'artifacts/fair_model.pkl'\n",
    "joblib.dump({\n",
    "    'model': best_model,\n",
    "    'model_name': f\"{best_name} (Fairness Constrained)\",\n",
    "    'threshold': recommended_threshold,\n",
    "    'note': 'Use with recommended_threshold for fairer outcomes'\n",
    "}, fair_model_file)\n",
    "print(f\"Saved {fair_model_file}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NOTEBOOK 03 COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAll artifacts saved to artifacts/ directory.\")\n",
    "print(\"See artifacts/fairness_report.csv for detailed fairness metrics.\")\n",
    "print(\"See artifacts/metrics_report.csv for model performance comparison.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
